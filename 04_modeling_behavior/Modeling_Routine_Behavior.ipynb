{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/chi2019_logo_final.png\">\n",
    "\n",
    "# Bayesian Methods in HCI\n",
    "\n",
    "\n",
    "$$\\newcommand{\\vec}[1]{{\\bf #1} } \n",
    "\\newcommand{\\real}{\\mathbb{R} }\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1] }\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "$$\n",
    "\n",
    "----\n",
    "\n",
    " **Nikola Banovic**\n",
    "\n",
    "* **University of Michigan**\n",
    "* nbanovic@umich.edu\n",
    "* [www.nikolabanovic.net](www.nikolabanovic.net)\n",
    "* @nikola_banovic  \n",
    "* [github.com/nbanovic](https://github.com/nbanovic)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# Modeling Human Behavior\n",
    "\n",
    "Models of human behavior enable us to explore and describe people’s cognition, behaviors, and environments in which they are situated. We define behaviors as sequences of situatoins people find themselves in and actions they perform in those situations.\n",
    "\n",
    "<img src=\"imgs/sequence.png\"/>\n",
    "\n",
    "Human behavior are thus physical actions and emotions that people exhibit. Although there are many kinds of behaviors, we will focus on modeling human routine behavior. We define routine behaviors as \"likely, weakly ordered, interruptible sequences of situations and actions that a person will perform in those situations to create or reach opportunities that enable the person to accomplish a goal\" ([Banovic et al, 2018](https://global.oup.com/academic/product/computational-interaction-9780198799603?cc=us&lang=en&)).\n",
    "\n",
    "Computational models, or mathematical representions of such complex systems of behaviors and environments, enable us to apply various computational modeling methods for exploring the systems by simulation and prediction. Such computational models and methods enable next generation of behavior-aware User Interfaces that can automatically reason about and act in response to people’s behaviors. In this module, we teach you how to create such computational models of human behavior. We will illustrate [a recent behavior modeling approach](http://dl.acm.org/authorize?N03134) based on Inverse Reinforcement Learning. We will use a Python implementation of Maximum Causal Entropy Inverse Reinforcement Learning from [Brian Ziebart's PhD thesis (2010)](http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf) borrowed from [krasheninnikov/max-causal-ent-irl](https://github.com/krasheninnikov/max-causal-ent-irl).\n",
    "\n",
    "Inverse Reinforcement Learning is particularly well suited for this problem because it allows us to mathematically  express our definition of human routine behavior and to capture and estimate the preference that people have for different goal situations.\n",
    "\n",
    "Here, we are going to follow a computaional modeling pipeline. However, our focus in this module will primarely be on building a computational model base on Inverse Reinforcement Learning.\n",
    "\n",
    "<img src=\"imgs/irl_modeling.png\"/>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports that we will need in the rest of the notebook.\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "\n",
    "# Discrete distributions and sampling\n",
    "from gym import Env, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "## MDP Refresher\n",
    "Inverse Reinforcement Learning uses a Markov Decision Process (MDP) to encode behaviors. Here we briefly summarize details from and how it applies to [modeling human behaviors](http://dl.acm.org/authorize?N03134). A Markov decision process is a tuple. It consists of a set of states representing situations  people find themselves in, and actions that a person can take in those situations. In addition, the model includes an action-dependent probability distribution for each state transition p(s'|s,a), which specifies the probability of the next state s'\n",
    " when the person performs action a in state s. This state transition probability distribution models how the environment responds to the actions that people perform in different\n",
    "states. When modeling human behavior, the transitions are often stochastic (each pair (s, a) can transition to many transition states s' with different probabilities). However, if the person has full control over the environment, they can also be deterministic (i.e., for each pair (s, a) there is exactly one transition state s' with probability 1). Finally, there is a reward function R that the person incurs when performing action a in state s, which represents the utility that people get from performing different actions in different contexts.\n",
    "\n",
    "<img src=\"imgs/mdp_model.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    '''\n",
    "    MDP object\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.nS : int\n",
    "        Number of states in the MDP.\n",
    "    self.nA : int\n",
    "        Number of actions in the MDP.\n",
    "    self.P : two-level dict of lists of tuples\n",
    "        First key is the state and the second key is the action.\n",
    "        self.P[state][action] is a list of tuples (prob, nextstate, reward).\n",
    "    self.T : 3D numpy array\n",
    "        The transition prob matrix of the MDP. p(s'|s,a) = self.T[s,a,s']\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        P, nS, nA, desc = MDP.env2mdp(env)\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means\n",
    "        self.env = env\n",
    "        self.T = self.get_transition_matrix()\n",
    "        self.s = self.reset()\n",
    "\n",
    "    def env2mdp(env):\n",
    "        return ({s : {a : [tup[:3] for tup in tups]\n",
    "                for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()},\n",
    "                env.nS, env.nA, env.desc)\n",
    "\n",
    "    def get_transition_matrix(self):\n",
    "        '''Return a matrix with index S,A,S' -> P(S'|S,A)'''\n",
    "        T = np.zeros([self.nS, self.nA, self.nS])\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                transitions = self.P[s][a]\n",
    "                s_a_s = {t[1]:t[0] for t in transitions}\n",
    "                for s_prime in range(self.nS):\n",
    "                    if s_prime in s_a_s:\n",
    "                        T[s, a, s_prime] = s_a_s[s_prime]\n",
    "        return T\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = 0\n",
    "        return self.s\n",
    "\n",
    "    def step(self, a, s=None):\n",
    "        if s == None: s = self.s\n",
    "        if len(self.P[s][a])==1:\n",
    "            self.s = self.P[s][a][0][1]\n",
    "            return self.s\n",
    "        else:\n",
    "            p_s_sa = np.asarray(self.P[s][a])[:,0]\n",
    "            next_state_index = np.random.choice(range(len(p_s_sa)), p=p_s_sa)\n",
    "            self.s = self.P[s][a][next_state_index][1]\n",
    "            return self.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "class DiscreteEnv(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary dict of dicts of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA, P, isd):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction=None # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self._seed()\n",
    "        self._reset()\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction=None\n",
    "        return self.s\n",
    "\n",
    "    def _step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d= transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction=a\n",
    "        return (s, r, d, {\"prob\" : p})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Model Definition and Feature Engineering\n",
    "\n",
    "<img src=\"imgs/feature_engineering.png\"/>\n",
    "\n",
    "Unlike a purely algorithmic approach, we seek to define behaviors based on our existing knowledge and hypotheses. Thus, we carefully pick features that describe the behavior we are interested in modeling. In this module, we will work with a simple example that models people's routine when deciding if they should wear a jacket or not when leaving home.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinkBeforeYouStepOut(DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Will you or won't you bring your jacket?\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.desc = 'TBYSO'\n",
    "        \n",
    "        weather_transition_matrix = np.matrix('0.9 0.1; 0.05 0.95')\n",
    "\n",
    "        feature_matrix = None\n",
    "        nA = 0\n",
    "        nS = 0\n",
    "\n",
    "        state_feature_names_dict = {'Weather':['Hot','Cold'],\n",
    "                                    'At Home':['True', 'False'], \n",
    "                                    'Has Jacket': ['True', 'False'],\n",
    "                                    'Holding Jacket':['True', 'False'], \n",
    "                                    'Wearing Jacket':['True', 'False'], \n",
    "                                    'Feeling':['Cold','Just Right', 'Hot']}\n",
    "\n",
    "        # Flatten.\n",
    "        self.state_feature_names = [key + ':' + value for key, value_list in state_feature_names_dict.items() for value in value_list]\n",
    "\n",
    "        action_feature_names_dict = {'Jacket':['Bring','Leave','On','Off']}\n",
    "\n",
    "        self.action_feature_names = [key + ':' + value for key, value_list in action_feature_names_dict.items() for value in value_list]\n",
    "\n",
    "        state_feature_num = len(self.state_feature_names)\n",
    "        action_feature_num = len(self.action_feature_names)\n",
    "\n",
    "        # Initial state indicator (1 when initial state, 0 otherwise)\n",
    "        isi = []\n",
    "\n",
    "        # Create all possible states (i.e., reject those that are not possible).\n",
    "        for weather in state_feature_names_dict['Weather']:\n",
    "            for at_home in state_feature_names_dict['At Home']:\n",
    "                for has_jacket in state_feature_names_dict['Has Jacket']:\n",
    "\n",
    "                    # If you are at home you always have a jacket.\n",
    "                    if at_home == 'True' and has_jacket == 'False':\n",
    "                        continue\n",
    "                        \n",
    "                    for holding_jacket in state_feature_names_dict['Holding Jacket']:\n",
    "                        \n",
    "                        # If you are at home you are not holding your jacket.\n",
    "                        if at_home == 'True' and holding_jacket == 'True':\n",
    "                            continue\n",
    "                            \n",
    "                        # If you are not at home you can hold the jacket only when you have it.\n",
    "                        if at_home == 'False' and has_jacket == 'False' and holding_jacket == 'True':\n",
    "                            continue\n",
    "\n",
    "                        for wearing_jacket in state_feature_names_dict['Wearing Jacket']:\n",
    "\n",
    "                            # Come on! Nobody wears jackets at home.\n",
    "                            if at_home == 'True' and wearing_jacket == 'True':\n",
    "                                continue\n",
    "\n",
    "                            # You can only wear jacket when you have it with you.\n",
    "                            if has_jacket == 'False' and wearing_jacket == 'True':\n",
    "                                continue\n",
    "                                \n",
    "                            # You cannot wear and hold the jacket and the same time.\n",
    "                            if holding_jacket == 'True' and wearing_jacket == 'True':\n",
    "                                continue\n",
    "                                \n",
    "                            # If you are outside and you have a jacket, you must either wearing it or hold it.\n",
    "                            if at_home == 'False' and has_jacket == 'True' and holding_jacket == 'False' and wearing_jacket == 'False':\n",
    "                                continue\n",
    "                                \n",
    "                            # If you are outside and you don't have a jacket, you can't hold it or wear it.\n",
    "                            if at_home == 'False' and has_jacket == 'False' and (holding_jacket == 'True' or wearing_jacket == 'True'):\n",
    "                                continue\n",
    "\n",
    "                            for feeling in state_feature_names_dict['Feeling']:\n",
    "\n",
    "                                # When at home you do not wear a jacket and feel just right.\n",
    "\n",
    "                                if at_home == 'True':\n",
    "                                    if not(feeling == 'Just Right'):\n",
    "                                        continue                            \n",
    "                                elif at_home == 'False':\n",
    "                                    if weather == 'Cold':\n",
    "                                        #If the weather is cold, you can feel just right only when wearing a jacket, and cold otherwise.\n",
    "                                        if wearing_jacket == 'True' and not(feeling == 'Just Right'):\n",
    "                                            continue\n",
    "                                        elif wearing_jacket == 'False' and not(feeling == 'Cold'):\n",
    "                                            continue\n",
    "\n",
    "                                    elif weather == 'Hot':\n",
    "                                        #If the weather is hot, you can feel just right only when not wearing a jacket, and hot otherwise.\n",
    "                                        if wearing_jacket == 'True' and not(feeling == 'Hot'):\n",
    "                                            continue\n",
    "                                        elif wearing_jacket == 'False' and not(feeling == 'Just Right'):\n",
    "                                            continue\n",
    "\n",
    "                                #  We are here, so it must mean it is a valid state because we haven't rejected it.\n",
    "                                current_state = np.array([weather == 'Hot', weather == 'Cold', at_home == 'True', at_home == 'False', has_jacket == 'True', has_jacket == 'False', holding_jacket == 'True', holding_jacket == 'False', wearing_jacket == 'True', wearing_jacket == 'False', feeling == 'Cold', feeling == 'Just Right', feeling == 'Hot'], dtype = int)\n",
    "\n",
    "                                print('State: ', ['Weather:'+weather, 'At Home:' + at_home, 'Has Jacket:' + has_jacket, 'Holding Jacket:' + holding_jacket, 'Wearing Jacket:' + wearing_jacket, 'Feeling:' + feeling])\n",
    "\n",
    "                                if feature_matrix is None:\n",
    "                                    feature_matrix = np.matrix(current_state).T\n",
    "                                else:\n",
    "                                    feature_matrix = np.concatenate((feature_matrix, np.matrix(current_state).T), axis=1)\n",
    "\n",
    "\n",
    "                                isi.append(at_home == 'True')\n",
    "\n",
    "                                nS = nS + 1\n",
    "\n",
    "\n",
    "         # Create all possible actions (i.e., reject those that are not possible).\n",
    "        for jacket in action_feature_names_dict['Jacket']:\n",
    "            #  There is one action for each feature.\n",
    "            nA = nA + 1\n",
    "\n",
    "        self.feature_matrix = feature_matrix.T\n",
    "        \n",
    "        #  Initial state distribution. Initial states are the one when you are at home before you leave.\n",
    "        isd = np.array(isi).astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        # What does isd tell us about probability of being hot or cold?\n",
    "        print(\"Initial state distribution: \", isd)\n",
    "\n",
    "        # Create transition matrix.\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        for from_state in range(nS):\n",
    "            for action in range(nA):\n",
    "                for to_state in range(nS):\n",
    "                    li = P[from_state][action]\n",
    "\n",
    "                    from_state_weather_cold = feature_matrix.item(1,from_state)\n",
    "                    from_state_at_home_true = feature_matrix.item(2,from_state)\n",
    "                    from_state_has_jacket_true = feature_matrix.item(4,from_state)\n",
    "                    from_state_holding_jacket_true = feature_matrix.item(6,from_state)\n",
    "                    from_state_wearing_jacket_true = feature_matrix.item(8,from_state)\n",
    "                    from_state_feeling_hot = feature_matrix.item(10,from_state)\n",
    "                    from_state_feeling_justright = feature_matrix.item(11,from_state)\n",
    "                    from_state_feeling_cold = feature_matrix.item(12,from_state)\n",
    "\n",
    "                    to_state_weather_cold = feature_matrix.item(1,to_state)\n",
    "                    to_state_at_home_true = feature_matrix.item(2,to_state)\n",
    "                    to_state_has_jacket_true = feature_matrix.item(4,to_state)\n",
    "                    to_state_holding_jacket_true = feature_matrix.item(6,to_state)\n",
    "                    to_state_wearing_jacket_true = feature_matrix.item(8,to_state)\n",
    "                    to_state_feeling_hot = feature_matrix.item(10,to_state)\n",
    "                    to_state_feeling_justright = feature_matrix.item(11,to_state)\n",
    "                    to_state_feeling_cold = feature_matrix.item(12,to_state)\n",
    "\n",
    "                    # Initialize transition probability\n",
    "                    p = 1.0\n",
    "\n",
    "                    # The weather can change and you have no control over this!\n",
    "                    p *= weather_transition_matrix.item(from_state_weather_cold, to_state_weather_cold)\n",
    "\n",
    "                    # Can only transition from home to outside and from outside to outside.\n",
    "                    if from_state_at_home_true == 1 and to_state_at_home_true == 1:\n",
    "                        # We cannot stay at home. Have to keep moving.\n",
    "                        p = 0.0\n",
    "\n",
    "                    if from_state_at_home_true == 0 and to_state_at_home_true == 1:\n",
    "                        # We cannot go home. Not in this example.\n",
    "                        p = 0.0\n",
    "\n",
    "                    # Where are you?\n",
    "                    if(from_state_at_home_true == 1):\n",
    "                        # You are inside.\n",
    "                        if action == 0:\n",
    "                            # Bring. That means in the next state you will have it but not wear it or p is 0.\n",
    "                            if to_state_has_jacket_true == 0 or to_state_wearing_jacket_true == 1:\n",
    "                                p = 0\n",
    "                        elif action == 1:\n",
    "                            # Leave. That means in the next state you will not have it and thus not be wearing it or p is 0.\n",
    "                            if to_state_has_jacket_true == 1:\n",
    "                                p = 0\n",
    "                        elif action == 2:\n",
    "                            # Put on. That means in the next state you will have it and wear it or p is 0.\n",
    "                            if to_state_has_jacket_true == 0 or to_state_wearing_jacket_true == 0:\n",
    "                                p = 0\n",
    "                        elif action == 3:\n",
    "                            # Can't take off at home because you are not wearing it.\n",
    "                            p = 0\n",
    "\n",
    "                    else:\n",
    "                        # You are outside.\n",
    "                        #If you have jacket you must have it with you no matter what you do and if you don't you can't get it.\n",
    "                        if not(from_state_has_jacket_true == to_state_has_jacket_true):\n",
    "                            p = 0\n",
    "                        else:\n",
    "                            if action == 0:\n",
    "                                # Bring. You cannot bring the jacket when you are outside.\n",
    "                                p = 0\n",
    "                            elif action == 1:\n",
    "                                # Leave. You cannot leave the jacket when you are outside.\n",
    "                                p = 0\n",
    "                            elif action == 2:\n",
    "                                # Put on  or keep on. That means in the next state you will have it and wear it, but only when you already have it, or p is 0.\n",
    "                                if from_state_has_jacket_true == 0 or to_state_has_jacket_true == 0 or to_state_wearing_jacket_true == 0:\n",
    "                                    p = 0\n",
    "                            elif action == 3:\n",
    "                                # Take off or keep off. That means that in the next state you will have it, but not wear it, but only  if you already had it with you.\n",
    "                                if to_state_wearing_jacket_true == 1:\n",
    "                                    p = 0\n",
    "\n",
    "                    # Suppose that the \"true\" reward is when you are feeeling just right. We do not use this reward in IRL, we learn it. In RL you would identify it using preference illicitation and calculate optimal behavior.\n",
    "                    r = to_state_feeling_justright\n",
    "\n",
    "                    li.append((p, to_state, r, False))\n",
    "\n",
    "        super(ThinkBeforeYouStepOut, self).__init__(nS, nA, P, isd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Exploring the Environment\n",
    "\n",
    "Let's explore the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  ['Weather:Hot', 'At Home:True', 'Has Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:False', 'Feeling:Just Right']\n",
      "State:  ['Weather:Hot', 'At Home:False', 'Has Jacket:True', 'Holding Jacket:True', 'Wearing Jacket:False', 'Feeling:Just Right']\n",
      "State:  ['Weather:Hot', 'At Home:False', 'Has Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:True', 'Feeling:Hot']\n",
      "State:  ['Weather:Hot', 'At Home:False', 'Has Jacket:False', 'Holding Jacket:False', 'Wearing Jacket:False', 'Feeling:Just Right']\n",
      "State:  ['Weather:Cold', 'At Home:True', 'Has Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:False', 'Feeling:Just Right']\n",
      "State:  ['Weather:Cold', 'At Home:False', 'Has Jacket:True', 'Holding Jacket:True', 'Wearing Jacket:False', 'Feeling:Cold']\n",
      "State:  ['Weather:Cold', 'At Home:False', 'Has Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:True', 'Feeling:Just Right']\n",
      "State:  ['Weather:Cold', 'At Home:False', 'Has Jacket:False', 'Holding Jacket:False', 'Wearing Jacket:False', 'Feeling:Cold']\n",
      "Initial state distribution:  [0.5 0.  0.  0.  0.5 0.  0.  0. ]\n",
      "State features:  ['Weather:Hot', 'Weather:Cold', 'At Home:True', 'At Home:False', 'Has Jacket:True', 'Has Jacket:False', 'Holding Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:True', 'Wearing Jacket:False', 'Feeling:Cold', 'Feeling:Just Right', 'Feeling:Hot']\n",
      "Action features:  ['Jacket:Bring', 'Jacket:Leave', 'Jacket:On', 'Jacket:Off']\n",
      "Transition probabilities: \n",
      "[[[0.   0.9  0.   0.   0.   0.1  0.   0.  ]\n",
      "  [0.   0.   0.   0.9  0.   0.   0.   0.1 ]\n",
      "  [0.   0.   0.9  0.   0.   0.   0.1  0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.9  0.   0.   0.   0.1  0.  ]\n",
      "  [0.   0.9  0.   0.   0.   0.1  0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.9  0.   0.   0.   0.1  0.  ]\n",
      "  [0.   0.9  0.   0.   0.   0.1  0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.9  0.   0.   0.   0.1 ]]\n",
      "\n",
      " [[0.   0.05 0.   0.   0.   0.95 0.   0.  ]\n",
      "  [0.   0.   0.   0.05 0.   0.   0.   0.95]\n",
      "  [0.   0.   0.05 0.   0.   0.   0.95 0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.05 0.   0.   0.   0.95 0.  ]\n",
      "  [0.   0.05 0.   0.   0.   0.95 0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.05 0.   0.   0.   0.95 0.  ]\n",
      "  [0.   0.05 0.   0.   0.   0.95 0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   0.05 0.   0.   0.   0.95]]]\n"
     ]
    }
   ],
   "source": [
    "environment = ThinkBeforeYouStepOut()\n",
    "mdp = MDP(environment)    \n",
    "\n",
    "print(\"State features: \", environment.state_feature_names)\n",
    "print(\"Action features: \", environment.action_feature_names)\n",
    "print(\"Transition probabilities: \")\n",
    "print(mdp.T)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##  Model Training\n",
    "\n",
    "<img src=\"imgs/irl_modeltraining.png\"/>\n",
    "\n",
    "Here, we will train an IRL model from demonstrated behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "###  Reinforcement Learning and Finding Optimal Behavior\n",
    "\n",
    "Unlike IRL that estimates a person's perference for certain situationsa and actions and computes a policy based on those estimated preferences, Reinforcement Learning (RL) computes an optimal policy (what they should do) given feedback about their preferences. The code below is for reference only, and you can use it to experiment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_rational(mdp, gamma, r, horizon=None, threshold=1e-16):\n",
    "    '''\n",
    "    Finds the optimal state and state-action value functions via value \n",
    "    iteration with the Bellman backup.\n",
    "    \n",
    "    Computes the rational policy \\pi_{s,a} = \\argmax(Q_{s,a}).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    gamma : float \n",
    "        Discount factor; 0<=gamma<=1.\n",
    "    r : 1D numpy array\n",
    "        Initial reward vector with the length equal to the \n",
    "        number of states in the MDP.\n",
    "    horizon : int\n",
    "        Horizon for the finite horizon version of value iteration.\n",
    "    threshold : float\n",
    "        Convergence threshold.\n",
    "    Returns\n",
    "    -------\n",
    "    1D numpy array\n",
    "        Array of shape (mdp.nS, 1), each V[s] is the value of state s under \n",
    "        the reward r and Boltzmann policy.\n",
    "    2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each Q[s,a] is the value of \n",
    "        state-action pair [s,a] under the reward r and Boltzmann policy.\n",
    "    2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each value p[s,a] is the probability \n",
    "        of taking action a in state s.\n",
    "    '''\n",
    "    \n",
    "    V = np.copy(r)\n",
    "\n",
    "    t = 0\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        V_prev = np.copy(V)\n",
    "        \n",
    "        # Q[s,a] = (r_s + gamma * \\sum_{s'} p(s'|s,a)V_{s'})\n",
    "        Q = r.reshape((-1,1)) + gamma * np.dot(mdp.T, V_prev)\n",
    "        # V_s = max_a(Q_sa)\n",
    "        V = np.amax(Q, axis=1)\n",
    "\n",
    "        diff = np.amax(abs(V_prev - V))\n",
    "        \n",
    "        t+=1\n",
    "        if horizon is not None:\n",
    "            if t==horizon: break\n",
    "    \n",
    "    V = V.reshape((-1, 1))\n",
    "\n",
    "    # Compute policy\n",
    "    # Assigns equal probability to taking actions whose Q_sa == max_a(Q_sa)\n",
    "    max_Q_index = (Q == np.tile(np.amax(Q,axis=1),(mdp.nA,1)).T)\n",
    "    policy = max_Q_index / np.sum(max_Q_index, axis=1).reshape((-1,1))\n",
    "\n",
    "    return V, Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Inverse Reinforcement Learning\n",
    "\n",
    "We use the [MaxCausalEnt](http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf) IRL algorithm to train our  model. The principle of Maximum Entropy ensures that the probability distribution of actions given states is the one that makes least amount of assumptions about the behaviors than what is present in the data.\n",
    "\n",
    "<img src=\"imgs/maxcausalent.png\"/>\n",
    "\n",
    "We maximize causal entropy by useing  softmax iteration.\n",
    "\n",
    "<img src=\"imgs/irl_softmax.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "Softmax allows us to compute a probability distribution over actions given situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, t=1):\n",
    "    '''\n",
    "    Numerically stable computation of t*log(\\sum_j^n exp(x_j / t))\n",
    "    \n",
    "    If the input is a 1D numpy array, computes it's softmax: \n",
    "        output = t*log(\\sum_j^n exp(x_j / t)).\n",
    "    If the input is a 2D numpy array, computes the softmax of each of the rows:\n",
    "        output_i = t*log(\\sum_j^n exp(x_{ij} / t))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D or 2D numpy array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    1D numpy array \n",
    "        shape = (n,), where: \n",
    "            n = 1 if x was 1D, or \n",
    "            n is the number of rows (=x.shape[0]) if x was 2D.\n",
    "    '''\n",
    "    assert t>=0\n",
    "    if len(x.shape) == 1: x = x.reshape((1,-1))\n",
    "    if t == 0: return np.amax(x, axis=1)\n",
    "    if x.shape[1] == 1: return x\n",
    "   \n",
    "    def softmax_2_arg(x1,x2, t):\n",
    "        ''' \n",
    "        Numerically stable computation of t*log(exp(x1/t) + exp(x2/t))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x1 : numpy array of shape (n,1)\n",
    "        x2 : numpy array of shape (n,1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array of shape (n,1)\n",
    "            Each output_i = t*log(exp(x1_i / t) + exp(x2_i / t))\n",
    "        '''\n",
    "        tlog = lambda x: t * np.log(x)\n",
    "        expt = lambda x: np.exp(x/t)\n",
    "                \n",
    "        max_x = np.amax((x1,x2),axis=0)\n",
    "        min_x = np.amin((x1,x2),axis=0)    \n",
    "        return max_x + tlog(1+expt((min_x - max_x)))\n",
    "    \n",
    "    sm = softmax_2_arg(x[:,0],x[:,1], t)\n",
    "    # Use the following property of softmax_2_arg:\n",
    "    # softmax_2_arg(softmax_2_arg(x1,x2),x3) = log(exp(x1) + exp(x2) + exp(x3))\n",
    "    # which is true since\n",
    "    # log(exp(log(exp(x1) + exp(x2))) + exp(x3)) = log(exp(x1) + exp(x2) + exp(x3))\n",
    "    for (i, x_i) in enumerate(x.T):\n",
    "        if i>1: sm = softmax_2_arg(sm, x_i, t)\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration and Policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_boltzmann(mdp, gamma, r, horizon=None,  temperature=1, \n",
    "                            threshold=1e-16):\n",
    "    '''\n",
    "    Finds the optimal state and state-action value functions via value \n",
    "    iteration with the \"soft\" max-ent Bellman backup:\n",
    "    \n",
    "    Q_{sa} = r_s + gamma * \\sum_{s'} p(s'|s,a)V_{s'}\n",
    "    V'_s = temperature * log(\\sum_a exp(Q_{sa}/temperature))\n",
    "    Computes the Boltzmann rational policy \n",
    "    \\pi_{s,a} = exp((Q_{s,a} - V_s)/temperature).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    gamma : float \n",
    "        Discount factor; 0<=gamma<=1.\n",
    "    r : 1D numpy array\n",
    "        Initial reward vector with the length equal to the \n",
    "        number of states in the MDP.\n",
    "    horizon : int\n",
    "        Horizon for the finite horizon version of value iteration.\n",
    "    threshold : float\n",
    "        Convergence threshold.\n",
    "    Returns\n",
    "    -------\n",
    "    1D numpy array\n",
    "        Array of shape (mdp.nS, 1), each V[s] is the value of state s under \n",
    "        the reward r and Boltzmann policy.\n",
    "    2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each Q[s,a] is the value of \n",
    "        state-action pair [s,a] under the reward r and Boltzmann policy.\n",
    "    2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each value p[s,a] is the probability \n",
    "        of taking action a in state s.\n",
    "    '''\n",
    "    \n",
    "    # No rewards for state-action pairs where there is no transition\n",
    "    mask = np.sum(mdp.T, axis=2)\n",
    "        \n",
    "    #Value iteration    \n",
    "    V = np.copy(r)\n",
    "    t = 0\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        V_prev = np.copy(V)\n",
    "\n",
    "        # ∀ s,a: Q[s,a] = (r_s + gamma * \\sum_{s'} p(s'|s,a)V_{s'})\n",
    "        Q = np.multiply(r.reshape((-1,1)) + gamma * np.dot(mdp.T, V_prev), mask)\n",
    "        \n",
    "        # ∀ s: V_s = temperature * log(\\sum_a exp(Q_sa/temperature))\n",
    "        V = softmax(Q, temperature)\n",
    "        \n",
    "        diff = np.amax(abs(V_prev - V))\n",
    "        \n",
    "        t+=1\n",
    "        if t<horizon and gamma==1:\n",
    "            # When \\gamma=1, the backup operator is equivariant under adding \n",
    "            # a constant to all entries of V, so we can translate min(V) \n",
    "            # to be 0 at each step of the softmax value iteration without \n",
    "            # changing the policy it converges to, and this fixes the problem \n",
    "            # where log(nA) keep getting added at each iteration.\n",
    "            V = V - np.amin(V)\n",
    "        if horizon is not None:\n",
    "            if t==horizon: break\n",
    "    \n",
    "    V = V.reshape((-1, 1))\n",
    "        \n",
    "    # Compute policy\n",
    "    expt = lambda x: np.exp(x/temperature)\n",
    "    tlog = lambda x: temperature * np.log(x)\n",
    "\n",
    "    # ∀ s,a: policy_{s,a} = exp((Q_{s,a} - V_s)/t)\n",
    "    policy = expt(Q - V)\n",
    "        \n",
    "    return V, Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing State Counts from Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_s_a_visitations(mdp, gamma, trajectories):\n",
    "    '''\n",
    "    Given a list of trajectories in an mdp, computes the state-action \n",
    "    visitation counts and the probability of a trajectory starting in state s.\n",
    "    \n",
    "    State-action visitation counts:\n",
    "    sa_visit_count[s,a] = \\sum_{i,t} 1_{traj_s_{i,t} = s AND traj_a_{i,t} = a}\n",
    "    P_0(s) -- probability that the trajectory will start in state s. \n",
    "    P_0[s] = \\sum_{i,t} 1_{t = 0 AND traj_s_{i,t} = s}  / i\n",
    "    P_0 is used in computing the occupancy measure of the MDP.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    gamma : float \n",
    "        Discount factor; 0<=gamma<=1.\n",
    "    trajectories : 3D numpy array\n",
    "        Expert trajectories. \n",
    "        Dimensions: [number of traj, timesteps in the traj, 2: state & action].\n",
    "    Returns\n",
    "    -------\n",
    "    (2D numpy array, 1D numpy array)\n",
    "        Arrays of shape (mdp.nS, mdp.nA) and (mdp.nS).\n",
    "    '''\n",
    "\n",
    "    s_0_count = np.zeros(mdp.nS)\n",
    "    sa_visit_count = np.zeros((mdp.nS, mdp.nA))\n",
    "    \n",
    "    for traj in trajectories:\n",
    "        # traj[0][0] is the state of the first timestep of the trajectory.\n",
    "        s_0_count[traj[0][0]] += 1\n",
    "        for (s, a) in traj:\n",
    "            sa_visit_count[s, a] += 1\n",
    "      \n",
    "    # Count into probability        \n",
    "    P_0 = s_0_count / trajectories.shape[0]\n",
    "    \n",
    "    return sa_visit_count, P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing State Counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_D(mdp, gamma, policy, P_0=None, t_max=None, threshold=1e-6):\n",
    "    '''\n",
    "    Computes occupancy measure of a MDP under a given time-constrained policy \n",
    "    -- the expected discounted number of times that policy π visits state s in \n",
    "    a given number of timesteps.\n",
    "    \n",
    "    The version w/o discount is described in Algorithm 9.3 of Ziebart's thesis: \n",
    "    http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf.\n",
    "    \n",
    "    The discounted version can be found in the supplement to Levine's \n",
    "    \"Nonlinear Inverse Reinforcement Learning with Gaussian Processes\" (GPIRL):\n",
    "    https://graphics.stanford.edu/projects/gpirl/gpirl_supplement.pdf.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    gamma : float \n",
    "        Discount factor; 0<=gamma<=1.\n",
    "    policy : 2D numpy array\n",
    "        policy[s,a] is the probability of taking action a in state s.\n",
    "    P_0 : 1D numpy array of shape (mdp.nS)\n",
    "        i-th element is the probability that the traj will start in state i.\n",
    "    t_max : int\n",
    "        number of timesteps the policy is executed.\n",
    "    Returns\n",
    "    -------\n",
    "    1D numpy array of shape (mdp.nS)\n",
    "    '''\n",
    "\n",
    "    if P_0 is None: P_0 = np.ones(mdp.nS) / mdp.nS\n",
    "    D_prev = np.zeros_like(P_0)     \n",
    "    \n",
    "    t = 0\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        \n",
    "        # ∀ s: D[s] <- P_0[s]\n",
    "        D = np.copy(P_0)\n",
    "\n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:\n",
    "                    D[s_prime] += gamma * D_prev[s] * policy[s, a] * p_sprime\n",
    "\n",
    "        diff = np.amax(abs(D_prev - D))    \n",
    "        D_prev = np.copy(D)\n",
    "        \n",
    "        if t_max is not None:\n",
    "            t+=1\n",
    "            if t==t_max: break\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it All Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_causal_ent_irl(mdp, feature_matrix, trajectories, gamma=1, h=None, \n",
    "                       temperature=1e-2, epochs=300, learning_rate=0.01, theta=None):\n",
    "    '''\n",
    "    Finds theta, a reward parametrization vector (r[s] = features[s]'.*theta) \n",
    "    that maximizes the log likelihood of the given expert trajectories, \n",
    "    modelling the expert as a Boltzmann rational agent with given temperature. \n",
    "    \n",
    "    This is equivalent to finding a reward parametrization vector giving rise \n",
    "    to a reward vector giving rise to Boltzmann rational policy whose expected \n",
    "    feature count matches the average feature count of the given expert \n",
    "    trajectories (Levine et al, supplement to the GPIRL paper).\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    feature_matrix : 2D numpy array\n",
    "        Each of the rows of the feature matrix is a vector of features of the \n",
    "        corresponding state of the MDP. \n",
    "    trajectories : 3D numpy array\n",
    "        Expert trajectories. \n",
    "        Dimensions: [number of traj, timesteps in the traj, state and action].\n",
    "    gamma : float \n",
    "        Discount factor; 0<=gamma<=1.\n",
    "    h : int\n",
    "        Horizon for the finite horizon version of value iteration.\n",
    "    temperature : float >= 0\n",
    "        The temperature parameter for computing V, Q and policy of the \n",
    "        Boltzmann rational agent: p(a|s) is proportional to exp(Q/temperature);\n",
    "        the closer temperature is to 0 the more rational the agent is.\n",
    "    epochs : int\n",
    "        Number of iterations gradient descent will run.\n",
    "    learning_rate : float\n",
    "        Learning rate for gradient descent.\n",
    "    theta : 1D numpy array\n",
    "        Initial reward function parameters vector with the length equal to the \n",
    "        #features.\n",
    "    Returns\n",
    "    -------\n",
    "    1D numpy array\n",
    "        Reward function parameters computed with Maximum Causal Entropy \n",
    "        algorithm from the expert trajectories.\n",
    "    '''    \n",
    "    \n",
    "    # Compute the state-action visitation counts and the probability \n",
    "    # of a trajectory starting in state s from the expert trajectories.\n",
    "    sa_visit_count, P_0 = compute_s_a_visitations(mdp, gamma, trajectories)\n",
    "    \n",
    "    # Mean state visitation count of expert trajectories\n",
    "    # mean_s_visit_count[s] = ( \\sum_{i,t} 1_{traj_s_{i,t} = s}) / num_traj\n",
    "    mean_s_visit_count = np.sum(sa_visit_count,1) / trajectories.shape[0]\n",
    "    # Mean feature count of expert trajectories\n",
    "    mean_f_count = np.dot(feature_matrix.T, mean_s_visit_count)\n",
    "    \n",
    "    if theta is None:\n",
    "        theta = np.random.rand(feature_matrix.shape[1])\n",
    "        \n",
    "\n",
    "    for i in range(epochs):\n",
    "        r = np.squeeze(np.asarray(np.dot(feature_matrix, theta.reshape(-1,1))))\n",
    "        # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "        V, Q, policy = vi_boltzmann(mdp, gamma, r, h, temperature)\n",
    "        \n",
    "        # IRL log likelihood term: \n",
    "        # L = 0; for all traj: for all (s, a) in traj: L += Q[s,a] - V[s]\n",
    "        L = np.sum(sa_visit_count * (Q - V))\n",
    "        \n",
    "        # The expected #times policy π visits state s in a given #timesteps.\n",
    "        D = compute_D(mdp, gamma, policy, P_0, t_max=trajectories.shape[1])        \n",
    "\n",
    "        # IRL log likelihood gradient w.r.t rewardparameters. \n",
    "        # Corresponds to line 9 of Algorithm 2 from the MaxCausalEnt IRL paper \n",
    "        # www.cs.cmu.edu/~bziebart/publications/maximum-causal-entropy.pdf. \n",
    "        # Negate to get the gradient of neg log likelihood, \n",
    "        # which is then minimized with GD.\n",
    "        dL_dtheta = -(mean_f_count - np.dot(feature_matrix.T, D))\n",
    "\n",
    "        # Gradient descent\n",
    "        theta = theta - learning_rate * dL_dtheta\n",
    "\n",
    "        if (i+1)%10==0: \n",
    "            print('Epoch: {} log likelihood of all traj: {}'.format(i,L), \n",
    "                  ', average per traj step: {}'.format(\n",
    "                  L/(trajectories.shape[0] * trajectories.shape[1])))\n",
    "    return theta, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Example Trajectories\n",
    "\n",
    "Normally, we would collect data from actual people, but here we will use syntetic data that we can generate by specifying a person's preferences, and then sample that person's behaviors from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectories(mdp, policy, timesteps=4, num_traj=50):\n",
    "    '''\n",
    "    Generates trajectories in the MDP given a policy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : object\n",
    "        Instance of the MDP class.\n",
    "    policy : 2D numpy array\n",
    "        Array of shape (mdp.nS, mdp.nA), each value p[s,a] is the probability \n",
    "        of taking action a in state s.\n",
    "    timesteps : int\n",
    "        Length of each of the generated trajectories.\n",
    "    num_traj : \n",
    "        Number of trajectories to generate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    3D numpy array\n",
    "        Expert trajectories. \n",
    "        Dimensions: [number of traj, timesteps in the traj, 2: state & action].\n",
    "    '''\n",
    "    \n",
    "    trajectories = np.zeros([num_traj, timesteps, 2]).astype(int)\n",
    "    \n",
    "    s = mdp.reset()\n",
    "    for i in range(num_traj):\n",
    "        for t in range(timesteps):\n",
    "            action = np.random.choice(range(mdp.nA), p=policy[s, :])\n",
    "            trajectories[i, t, :] = [s, action]\n",
    "            s = mdp.step(action)\n",
    "        s = mdp.reset()\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State feature names:  ['Weather:Hot', 'Weather:Cold', 'At Home:True', 'At Home:False', 'Has Jacket:True', 'Has Jacket:False', 'Holding Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:True', 'Wearing Jacket:False', 'Feeling:Cold', 'Feeling:Just Right', 'Feeling:Hot']\n",
      "My  theta:  [0.4  0.95 0.3  0.95 0.99 0.2  0.4  0.35 0.9  0.55 0.5  0.7  0.2 ]\n",
      "My policy:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Generated 200 traj of length 4\n",
      "Log likelihood of all traj under the policy generated  from the true reward: 0.0, \n",
      " average per traj step: 0.0\n",
      "Average return per expert trajectory: 15.68125 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizon=4\n",
    "t_expert=1e-16\n",
    "gamma = 1\n",
    "n_traj=200\n",
    "traj_len = 4\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Features\n",
    "feature_matrix = environment.feature_matrix\n",
    "\n",
    "print('State feature names: ',  environment.state_feature_names)\n",
    "# The \"true\" reward weights and the reward\n",
    "theta_expert = np.array([0.40,0.95,0.30,0.95,0.99,0.20,0.40,0.35,0.90,0.55,0.50,0.70,0.20])\n",
    "r_expert = np.squeeze(np.asarray(np.dot(feature_matrix, theta_expert)))\n",
    "\n",
    "# Compute the Boltzmann rational expert policy from the given true reward.\n",
    "if t_expert>0:\n",
    "    V, Q, policy_expert = vi_boltzmann(mdp, gamma, r_expert, horizon, t_expert)\n",
    "if t_expert==0:\n",
    "    V, Q, policy_expert = vi_rational(mdp, gamma, r_expert, horizon)\n",
    "\n",
    "print(\"My  theta: \", theta_expert)\n",
    "print(\"My policy:\")\n",
    "print(policy_expert)\n",
    "\n",
    "# Generate expert trajectories using the given expert policy.\n",
    "trajectories = generate_trajectories(mdp, policy_expert, traj_len, n_traj)\n",
    "\n",
    "# Compute and print the stats of the generated expert trajectories.\n",
    "sa_visit_count, _ = compute_s_a_visitations(mdp, gamma, trajectories)\n",
    "\n",
    "log_likelihood = np.sum(sa_visit_count * (Q - V))\n",
    "print('Generated {} traj of length {}'.format(n_traj, traj_len))\n",
    "print('Log likelihood of all traj under the policy generated ', \n",
    "      'from the true reward: {}, \\n average per traj step: {}'.format(\n",
    "       log_likelihood, log_likelihood / (n_traj * traj_len)))\n",
    "print('Average return per expert trajectory: {} \\n'.format(\n",
    "        np.sum(np.sum(sa_visit_count, axis=1)*r_expert) / n_traj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Think Before you Step Out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State feature names:  ['Weather:Hot', 'Weather:Cold', 'At Home:True', 'At Home:False', 'Has Jacket:True', 'Has Jacket:False', 'Holding Jacket:True', 'Holding Jacket:False', 'Wearing Jacket:True', 'Wearing Jacket:False', 'Feeling:Cold', 'Feeling:Just Right', 'Feeling:Hot']\n",
      "Action feature names:  ['Jacket:Bring', 'Jacket:Leave', 'Jacket:On', 'Jacket:Off']\n",
      "Epoch: 9 log likelihood of all traj: -117.41452658115287 , average per traj step: -0.1467681582264411\n",
      "Epoch: 19 log likelihood of all traj: -91.80447783115261 , average per traj step: -0.11475559728894076\n",
      "Epoch: 29 log likelihood of all traj: -66.19442908115263 , average per traj step: -0.0827430363514408\n",
      "Epoch: 39 log likelihood of all traj: -40.58438033148567 , average per traj step: -0.05073047541435709\n",
      "Epoch: 49 log likelihood of all traj: -14.974745768624377 , average per traj step: -0.01871843221078047\n",
      "Epoch: 59 log likelihood of all traj: -0.8326094377267106 , average per traj step: -0.0010407617971583882\n",
      "Epoch: 69 log likelihood of all traj: -0.4782106377499957 , average per traj step: -0.0005977632971874946\n",
      "Epoch: 79 log likelihood of all traj: -0.4112281059456748 , average per traj step: -0.0005140351324320935\n",
      "Epoch: 89 log likelihood of all traj: -0.3892114085485332 , average per traj step: -0.0004865142606856665\n",
      "Epoch: 99 log likelihood of all traj: -0.3805849611077816 , average per traj step: -0.000475731201384727\n",
      "Epoch: 109 log likelihood of all traj: -0.37696421540898584 , average per traj step: -0.0004712052692612323\n",
      "Epoch: 119 log likelihood of all traj: -0.37539989591710565 , average per traj step: -0.00046924986989638207\n",
      "Epoch: 129 log likelihood of all traj: -0.37471553638516486 , average per traj step: -0.0004683944204814561\n",
      "Epoch: 139 log likelihood of all traj: -0.3744144991276688 , average per traj step: -0.000468018123909586\n",
      "Epoch: 149 log likelihood of all traj: -0.37428175907377526 , average per traj step: -0.00046785219884221905\n",
      "Epoch: 159 log likelihood of all traj: -0.3742231661849118 , average per traj step: -0.00046777895773113975\n",
      "Epoch: 169 log likelihood of all traj: -0.3741972905265163 , average per traj step: -0.00046774661315814535\n",
      "Epoch: 179 log likelihood of all traj: -0.37418586101171325 , average per traj step: -0.00046773232626464155\n",
      "Epoch: 189 log likelihood of all traj: -0.3741808120285164 , average per traj step: -0.0004677260150356455\n",
      "Epoch: 199 log likelihood of all traj: -0.37417858155238726 , average per traj step: -0.00046772322694048406\n",
      "Epoch: 209 log likelihood of all traj: -0.3741775961833609 , average per traj step: -0.0004677219952292011\n",
      "Epoch: 219 log likelihood of all traj: -0.37417716086805974 , average per traj step: -0.00046772145108507466\n",
      "Epoch: 229 log likelihood of all traj: -0.3741769685541332 , average per traj step: -0.0004677212106926665\n",
      "Epoch: 239 log likelihood of all traj: -0.37417688359356394 , average per traj step: -0.0004677211044919549\n",
      "Epoch: 249 log likelihood of all traj: -0.3741768460598882 , average per traj step: -0.0004677210575748603\n",
      "Epoch: 259 log likelihood of all traj: -0.3741768294777321 , average per traj step: -0.0004677210368471652\n",
      "Epoch: 269 log likelihood of all traj: -0.37417682215229053 , average per traj step: -0.0004677210276903632\n",
      "Epoch: 279 log likelihood of all traj: -0.37417681891611476 , average per traj step: -0.00046772102364514345\n",
      "Epoch: 289 log likelihood of all traj: -0.37417681748640064 , average per traj step: -0.0004677210218580008\n",
      "Epoch: 299 log likelihood of all traj: -0.3741768168549502 , average per traj step: -0.00046772102106868775\n",
      "Final reward weights:  [[0.400921   0.92987646 0.30526701 0.94306226 0.99065169 0.19889222\n",
      "  0.40584697 0.35748669 0.90190538 0.57632186 0.76837633 0.5168362\n",
      "  0.19968551]]\n",
      "Final policy:  [[9.65664608e-001 2.43054603e-147 3.43353916e-002 7.48283316e-295]\n",
      " [0.00000000e+000 0.00000000e+000 3.43353916e-002 9.65664608e-001]\n",
      " [6.42285340e-323 6.42285340e-323 3.43353916e-002 9.65664608e-001]\n",
      " [1.49604104e-141 1.49604104e-141 1.49604104e-141 1.00000000e+000]\n",
      " [9.61930974e-002 1.56213173e-150 9.03806903e-001 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 9.03806903e-001 9.61930974e-002]\n",
      " [0.00000000e+000 0.00000000e+000 9.03806903e-001 9.61930974e-002]\n",
      " [1.39794105e-266 1.39794105e-266 1.39794105e-266 1.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "# Demonstrates the usage of the implemented MaxCausalEnt IRL algorithm. \n",
    "    \n",
    "#     First a number of expert trajectories is generated using the true reward \n",
    "#     giving rise to the Boltzmann rational expert policy with temperature t_exp. \n",
    "    \n",
    "#     Hereafter the max_causal_ent_irl() function is used to find a reward vector\n",
    "#     that maximizes the log likelihood of the generated expert trajectories, \n",
    "#     modelling the expert as a Boltzmann rational agent with temperature t_irl.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     t_expert : float >= 0\n",
    "#         The temperature parameter for computing V, Q and policy of the \n",
    "#         Boltzmann rational expert: p(a|s) is proportional to exp(Q/t_expert);\n",
    "#         the closer temperature is to 0 the more rational the expert is.\n",
    "#     t_irl : float\n",
    "#         Temperature of the Boltzmann rational policy the IRL algorithm assumes\n",
    "#         the expert followed when generating the trajectories.\n",
    "#     gamma : float \n",
    "#         Discount factor; 0<=gamma<=1.\n",
    "#     h : int\n",
    "#         Horizon for the finite horizon version of value iteration subroutine of\n",
    "#         MaxCausalEnt IRL algorithm.\n",
    "#     n_traj : int\n",
    "#         Number of expert trajectories generated.\n",
    "#     traj_len : int\n",
    "#         Number of timesteps in each of the expert trajectories.\n",
    "#     learning_rate : float\n",
    "#         Learning rate for gradient descent in the MaxCausalEnt IRL algorithm.\n",
    "#     epochs : int\n",
    "#         Number of gradient descent steps in the MaxCausalEnt IRL algorithm.\n",
    "\n",
    "\n",
    "print('State feature names: ', environment.state_feature_names)\n",
    "print('Action feature names: ', environment.action_feature_names)\n",
    "\n",
    "# Find a reward vector that maximizes the log likelihood of the generated \n",
    "# expert trajectories.\n",
    "theta, policy = max_causal_ent_irl(mdp, feature_matrix, trajectories, h=horizon)\n",
    "print('Final reward weights: ', theta)\n",
    "print('Final policy: ', policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What have we done?\n",
    "\n",
    "This is a very simple model for a very simple problem that illustrates using IRL to model people's behaviors from  behavior traces.\n",
    "\n",
    "* **Defined  behaviors**: .\n",
    "* **Mathematically expressed those behaviors**: .\n",
    "* **Captured the uncertainity of human behavior**: .\n",
    "* **Estimated probability distribution of different behaviors given context from data**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
